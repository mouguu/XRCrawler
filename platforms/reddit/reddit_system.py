#!/usr/bin/env python3
"""
Reddit Data System - Unified Interface

A comprehensive system for scraping, storing, and analyzing Reddit data.
"""

import os
import sys
import json
import csv
from datetime import datetime
from typing import Optional, Dict, List, Any

# Add parent directory to path
sys.path.append(os.path.dirname(__file__))

# Import existing modules
from local_storage import local_data_manager
from reddit_standardizer import RedditStandardizer


class RedditSystem:
    """Redditæ•°æ®ç³»ç»Ÿ - ç»Ÿä¸€æ¥å£"""
    
    def __init__(self):
        """Initialize Reddit system"""
        self.standardizer = RedditStandardizer()
        self.db = local_data_manager
        print("ğŸš€ Reddit System initialized")
        
    def get_system_status(self) -> Dict[str, Any]:
        """Get current system status with retry mechanism"""
        import time

        # Check local storage
        total_posts = self.db.get_total_posts_count()
        db_connected = True

        return {
            'database_connected': db_connected,
            'total_posts_in_db': total_posts,
            'standardizer_ready': True,
            'system_ready': db_connected
        }
    
    def scrape_posts(self, target: str, target_count: int = 100, strategy: str = 'auto', save_json: bool = False) -> Dict[str, Any]:
        """
        Scrape Reddit posts using the modular scraper

        Args:
            target: Target subreddit (r/name) or user (u/name)
            target_count: Target number of posts
            strategy: Scraping strategy ('auto', 'hot', 'new', etc.)
            save_json: Whether to save individual JSON files
        """
        print(f"ğŸ”„ Starting scraping process (target: {target}, count: {target_count})")

        try:
            from scraper import scrape_reddit

            # Auto-determine strategy based on target count if auto
            if strategy == 'auto':
                if target_count > 2000:
                    strategy = 'super_full'
                else:
                    strategy = 'new'

            print("ğŸš€ Initializing modular scraper...")
            print(f"ğŸ“Š Configuration: {target}, {target_count} posts, {strategy} strategy")
            print("-" * 50)

            # ç›´æ¥è°ƒç”¨çˆ¬è™«çš„æ ¸å¿ƒåŠŸèƒ½
            result = scrape_reddit(
                target=target,
                max_posts=target_count,
                sort_type=strategy
            )

            if result['status'] == 'success':
                print("\n" + "=" * 50)
                print("âœ… Scraping process completed!")
                print(f"ğŸ“Š Results: {result['scraped_count']} posts scraped")
                print("ğŸ“Š Returning to main system...")
                return {
                    'status': 'success',
                    'message': 'Scraping completed successfully',
                    'posts_scraped': result['scraped_count']
                }
            else:
                print(f"\nâŒ Scraping failed: {result.get('message', 'Unknown error')}")
                return {'status': 'error', 'message': result.get('message', 'Unknown error')}

        except Exception as e:
            return {'status': 'error', 'message': f'Scraping failed: {str(e)}'}

    def export_professional_csv(self, output_file: str = None, quality_filter: str = 'all') -> Dict[str, Any]:
        """
        Export data to professional-grade CSV for Kaggle/research use

        Args:
            output_file: Output CSV filename
            quality_filter: 'all', 'high_quality', 'medium_plus', 'exclude_low'
        """
        if not output_file:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"uoft_reddit_dataset_{timestamp}.csv"

        print(f"ğŸ“Š Exporting professional dataset to {output_file}")
        print(f"ğŸ¯ Quality filter: {quality_filter}")

        try:
            # Memory-efficient streaming processing
            exported_count = 0
            processed_count = 0
            batch_size = 500  # Smaller batch size for memory efficiency
            offset = 0

            # Define professional column order for the dataset
            fieldnames = [
                # Core identifiers
                'post_id', 'title', 'content', 'author', 'subreddit',

                # Timestamps
                'created_utc', 'created_date',

                # Reddit metrics
                'score', 'upvote_ratio', 'num_comments',

                # Comment data
                'comments_json',

                # Quality metrics
                'quality_title', 'quality_content', 'quality_engagement',
                'quality_overall', 'quality_recommendation',

                # Boolean flags
                'include_in_dataset',

                # Content classification
                'content_type', 'has_discussion',

                # Direct URL
                'url',

                # Calculated engagement metric
                'engagement_score'
            ]

            # Open file for streaming write
            with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
                writer.writeheader()

                print("ğŸ”„ Starting memory-efficient streaming export...")

                # Iterate over all local posts
                batch_to_write = []
                for post_data in self.db.get_all_posts_iterator():
                    try:
                        post_data['created_date'] = self._convert_timestamp(post_data.get('created_utc'))

                        # Standardize the post
                        standardized_post = self.standardizer.create_dataset_ready_row(post_data)

                        # Apply quality filter
                        if self._passes_quality_filter(standardized_post, quality_filter):
                            batch_to_write.append(standardized_post)
                            processed_count += 1

                        # Write batch if full
                        if len(batch_to_write) >= batch_size:
                            writer.writerows(batch_to_write)
                            exported_count += len(batch_to_write)
                            print(f"   âœ… Batch: {len(batch_to_write)} exported, {exported_count} total exported")
                            batch_to_write.clear()

                    except Exception as e:
                        print(f"âš ï¸  Error processing post {post_data.get('id', 'unknown')}: {e}")
                        continue

                # Write remaining
                if batch_to_write:
                    writer.writerows(batch_to_write)
                    exported_count += len(batch_to_write)

            print(f"ğŸ’¾ Streaming export completed! {exported_count} posts exported from {processed_count} processed")

            if exported_count == 0:
                return {'status': 'error', 'message': 'No posts found matching criteria'}

            # Generate lightweight summary (without loading all data into memory)
            summary = self._generate_streaming_summary(output_file, exported_count, processed_count, quality_filter)

            return {
                'status': 'success',
                'output_file': output_file,
                'total_posts': exported_count,
                'processed_posts': processed_count,
                'summary': summary
            }

        except Exception as e:
            return {'status': 'error', 'message': f'Export failed: {str(e)}'}

    def _generate_streaming_summary(self, output_file: str, exported_count: int, processed_count: int, quality_filter: str) -> Dict[str, Any]:
        """Generate comprehensive summary for streaming export without loading all data into memory"""

        # Core export information
        export_info = {
            'output_file': output_file,
            'export_timestamp': datetime.now().isoformat(),
            'quality_filter': quality_filter,
            'exported_posts': exported_count,
            'processed_posts': processed_count,
            'filter_efficiency': f"{(exported_count/max(processed_count, 1)*100):.1f}%",
            'memory_efficient_export': True,
            'streaming_processing': True
        }

        # Essential data dictionary (static information)
        data_dictionary = {
            "dataset_info": {
                "name": "Reddit Dataset",
                "description": "A comprehensive dataset of Reddit posts and comments",
                "collection_method": "Reddit API via PRAW and JSON endpoints",
                "preprocessing": "Automated cleaning, standardization, and quality assessment",
                "exported_posts": exported_count,
                "export_date": datetime.now().strftime("%Y-%m-%d")
            },
            "key_columns": {
                "post_id": "Unique Reddit post identifier",
                "title": "Fully cleaned post title (HTML decoded, placeholders removed)",
                "content": "Fully cleaned post content/body text",
                "comments_json": "Structured JSON array of comment tree with full metadata",
                "quality_overall": "Composite quality score (0-10 scale)",
                "engagement_score": "Calculated metric: score + (scraped_comments * 2)",
                "url": "Direct URL to Reddit post"
            },
            "quality_system": {
                "scoring_method": "Automated algorithmic assessment (not human-reviewed)",
                "categories": "high_quality (â‰¥7.0), medium_quality (5.0-6.9), low_quality (3.0-4.9), exclude (<3.0)",
                "purpose": "Dataset curation and filtering"
            },
            "usage_notes": {
                "complete_documentation": "See DATA_DICTIONARY.md for detailed field descriptions",
                "high_quality_filter": "Use quality_recommendation == 'high_quality' for premium dataset",
                "comment_analysis": "Parse comments_json for structured comment data with metadata",
                "time_series": "Use created_utc for temporal analysis"
            }
        }

        # Combine all information
        complete_summary = {
            'export_info': export_info,
            'data_dictionary': data_dictionary,
            'technical_notes': {
                'memory_efficient': 'Exported using streaming processing to handle large datasets',
                'batch_processing': 'Data processed in small batches to minimize memory usage',
                'scalability': 'Can handle datasets of any size without memory constraints'
            }
        }

        # Save comprehensive summary to JSON file
        summary_file = output_file.replace('.csv', '_summary.json')
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(complete_summary, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“‹ Comprehensive summary saved to {summary_file}")

        return complete_summary

    def _convert_timestamp(self, timestamp):
        """Convert timestamp to readable format"""
        if timestamp:
            try:
                return datetime.fromtimestamp(float(timestamp)).strftime('%Y-%m-%d %H:%M:%S')
            except:
                return ""
        return ""
    
    def _passes_quality_filter(self, post: Dict, filter_type: str) -> bool:
        """Check if post passes quality filter"""
        if filter_type == 'all':
            return True
        elif filter_type == 'high_quality':
            return post.get('quality_recommendation') == 'high_quality'
        elif filter_type == 'medium_plus':
            return post.get('quality_recommendation') in ['high_quality', 'medium_quality']
        elif filter_type == 'exclude_low':
            return post.get('quality_recommendation') != 'exclude'
        else:
            return True
    



    
    def analyze_data_quality(self) -> Dict[str, Any]:
        """Analyze the quality of data in the database"""
        print("ğŸ” Analyzing data quality...")
        
        try:
            # Sample posts for analysis
            count = 0
            quality_scores = []
            content_types = {'text': 0, 'link': 0, 'empty': 0}
            
            for post_data in self.db.get_all_posts_iterator():
                if count >= 100:
                    break
                
                # Standardize and assess quality
                standardized = self.standardizer.create_dataset_ready_row(post_data)
                quality_scores.append(standardized.get('quality_overall', 0))
                
                content_type = standardized.get('content_type', 'unknown')
                if content_type in content_types:
                    content_types[content_type] += 1
                
                count += 1
            
            if count == 0:
                return {'status': 'error', 'message': 'No data found'}
            
            avg_quality = sum(quality_scores) / len(quality_scores) if quality_scores else 0
            
            analysis = {
                'status': 'success',
                'sample_size': len(result.data),
                'average_quality_score': avg_quality,
                'content_distribution': content_types,
                'quality_recommendation': 'Good' if avg_quality > 6 else 'Fair' if avg_quality > 4 else 'Needs improvement'
            }
            
            return analysis
            
        except Exception as e:
            return {'status': 'error', 'message': f'Analysis failed: {str(e)}'}
    
    def interactive_menu(self):
        """Interactive command-line interface"""
        while True:
            print("\n" + "="*60)
            print("ğŸ¯ Reddit Data System")
            print("="*60)
            
            # Show system status
            status = self.get_system_status()
            print(f"ğŸ“Š Database: {'âœ… Connected' if status['database_connected'] else 'âŒ Disconnected'}")
            print(f"ğŸ“ˆ Total Posts: {status['total_posts_in_db']}")
            print(f"ğŸ”§ System: {'âœ… Ready' if status['system_ready'] else 'âŒ Not Ready'}")
            
            print("\nAvailable Actions:")
            print("1. ğŸ”„ Scrape new posts")
            print("2. ğŸ“Š Export professional CSV dataset")
            print("3. ğŸ” Analyze data quality")
            print("4. ğŸ“Š Show system status")
            print("5. âŒ Exit")

            choice = input("\nSelect action (1-5): ").strip()
            
            if choice == '1':
                print("\nğŸ”„ Reddit Post Scraping Configuration")
                print("-" * 50)

                current_count = self.get_system_status()['total_posts_in_db']
                print(f"ğŸ“Š Current posts in database: {current_count}")

                # ç»Ÿä¸€çš„ç”¨æˆ·è¾“å…¥ç•Œé¢
                try:
                    # ç›®æ ‡é€‰æ‹©
                    target_input = input(f"ç›®æ ‡ (ä¾‹å¦‚ r/python æˆ– u/spez) [r/UofT]: ").strip()
                    target = target_input if target_input else "r/UofT"

                    # ç›®æ ‡æ•°é‡
                    default_target = 100
                    target_count_input = input(f"ç›®æ ‡å¸–å­æ•°é‡ [100]: ").strip()
                    target_count = int(target_count_input) if target_count_input.isdigit() else default_target

                    # ç­–ç•¥é€‰æ‹©
                    print("\nğŸ¯ ç­–ç•¥é€‰æ‹©:")
                    print("  1. auto - è‡ªåŠ¨é€‰æ‹© (æ¨è)")
                    print("  2. super_full - å…¨é¢æ¨¡å¼ (é€‚åˆå¤§é‡æŠ“å–)")
                    print("  3. super_recent - æ—¶æ•ˆä¼˜å…ˆ")
                    print("  4. new - æ™®é€šæ–°å¸–æ¨¡å¼")
                    strategy_choice = input("è¯·é€‰æ‹©ç­–ç•¥ (1-4) [1]: ").strip() or "1"
                    strategy_map = {"1": "auto", "2": "super_full", "3": "super_recent", "4": "new"}
                    strategy = strategy_map.get(strategy_choice, "auto")

                    # JSONä¿å­˜é€‰é¡¹
                    save_json = input("æ˜¯å¦ä¿å­˜JSONæ–‡ä»¶? (y/n) [n]: ").lower().startswith('y')

                    print(f"\nğŸš€ é…ç½®ç¡®è®¤:")
                    print(f"  ğŸ¯ ç›®æ ‡: {target}")
                    print(f"  ğŸ“Š æ•°é‡: {target_count}")
                    print(f"  âš™ï¸  ç­–ç•¥: {strategy}")
                    print(f"  ğŸ’¾ JSON: {'æ˜¯' if save_json else 'å¦'}")

                    confirm = input("\nç¡®è®¤å¼€å§‹æŠ“å–? (y/n) [y]: ").strip()
                    if confirm.lower() in ['', 'y', 'yes']:
                        result = self.scrape_posts(target=target, target_count=target_count, strategy=strategy, save_json=save_json)

                        if result['status'] == 'success':
                            print(f"âœ… {result['message']}")
                            if 'posts_scraped' in result:
                                print(f"ğŸ“ˆ æ–°å¢å¸–å­: {result['posts_scraped']}")
                        else:
                            print(f"âŒ {result['message']}")
                    else:
                        print("âŒ ç”¨æˆ·å–æ¶ˆæ“ä½œ")

                except ValueError:
                    print("âŒ è¾“å…¥æ— æ•ˆï¼Œè¯·è¾“å…¥æœ‰æ•ˆæ•°å­—")
                except KeyboardInterrupt:
                    print("\nâŒ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
                
            elif choice == '2':
                quality_filter = input("Quality filter (all/high_quality/medium_plus/exclude_low) [all]: ").strip() or 'all'
                result = self.export_professional_csv(quality_filter=quality_filter)
                if result['status'] == 'success':
                    print(f"âœ… Professional CSV exported: {result['output_file']}")
                    print(f"ğŸ“Š Total posts: {result['total_posts']}")
                    print(f"ğŸ“‹ Summary saved as: {result['output_file'].replace('.csv', '_summary.json')}")
                    print(f"ğŸ“– Data dictionary available in summary file")
                else:
                    print(f"âŒ Export failed: {result['message']}")

            elif choice == '3':
                result = self.analyze_data_quality()
                if result['status'] == 'success':
                    print(f"ğŸ“Š Sample size: {result['sample_size']}")
                    print(f"â­ Average quality: {result['average_quality_score']:.1f}/10")
                    print(f"ğŸ“‹ Content distribution: {result['content_distribution']}")
                    print(f"ğŸ’¡ Recommendation: {result['quality_recommendation']}")
                else:
                    print(f"âŒ Analysis failed: {result['message']}")

            elif choice == '4':
                status = self.get_system_status()
                print(json.dumps(status, indent=2))

            elif choice == '5':
                print("ğŸ‘‹ Goodbye!")
                break

            else:
                print("âŒ Invalid choice. Please select 1-5.")

def main():
    """Main entry point - Enhanced with better error handling"""
    print("ğŸš€ Reddit System")
    print("="*50)
    print("ğŸ“Š University of Toronto Reddit Data System")
    print("ğŸ¯ Professional dataset creation and analysis")
    print("="*50)

    try:
        print("ğŸ”„ Initializing system...")
        system = RedditSystem()
        print("âœ… System ready!\n")
        system.interactive_menu()

    except ImportError as e:
        print(f"âŒ Import error: {e}")
        print("ğŸ’¡ Please ensure all dependencies are installed:")
        print("   pip install -r requirements.txt")
        print("ğŸ’¡ Make sure all system files are in the same directory")

    except KeyboardInterrupt:
        print("\nğŸ‘‹ System interrupted by user")

    except Exception as e:
        print(f"âŒ System error: {e}")
        print("ğŸ’¡ Please check your configuration and try again")

if __name__ == "__main__":
    main()
