"""
Reddit Scraper - Simplified Main Class

General-purpose Reddit scraper with modular architecture.
Replaces the bloated EnhancedUofTScraper.
"""

import re
import time
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime

from core.rate_limiter import SmartRateController
from core.session_manager import SessionManager
from scrapers.user_scraper import UserScraper
from scrapers.subreddit_scraper import SubredditScraper
from local_storage import local_data_manager
from post_scraper import RedditPostScraper as PostScraper


class RedditScraper:
    """Reddité€šç”¨çˆ¬è™« - ç®€åŒ–ç‰ˆ"""
    
    def __init__(self, target: str):
        """
        Initialize Reddit scraper
        
        Args:
            target: Either a subreddit name (r/subreddit) or user (u/username)
                   Can be URL, shorthand (r/python), or just the name
        """
        # Parse target to determine mode
        self.is_user_mode = False
        self.target_name = target
        
        # Handle full URLs
        if "reddit.com" in target:
            if "/user/" in target:
                self.is_user_mode = True
                parts = target.split("/user/")
                if len(parts) > 1:
                    self.target_name = parts[1].split("/")[0]
            elif "/r/" in target:
                self.is_user_mode = False
                parts = target.split("/r/")
                if len(parts) > 1:
                    self.target_name = parts[1].split("/")[0]
        # Handle "u/Username" or "r/Subreddit" format
        elif target.startswith("u/") or target.startswith("user/"):
            self.is_user_mode = True
            self.target_name = target.split("/")[-1]
        elif target.startswith("r/"):
            self.is_user_mode = False
            self.target_name = target.split("/")[-1]
            
        print(f"ğŸ¯ ç›®æ ‡: {'ç”¨æˆ·' if self.is_user_mode else 'Subreddit'} {self.target_name}")
        
        # Initialize core components
        self.rate_controller = SmartRateController()
        self.session_manager = SessionManager()
        
        # Initialize scrapers
        self.user_scraper = UserScraper(self.session_manager, self.rate_controller)
        self.subreddit_scraper = SubredditScraper(self.session_manager, self.rate_controller)
        self.post_scraper = PostScraper()
        
        # Counters
        self.scraped_count = 0
        self.skipped_count = 0
        self.error_count = 0
    
    def scrape(
        self,
        max_posts: int = 100,
        sort_type: str = 'hot',
        keywords: Optional[List[str]] = None,
        progress_callback: Optional[Callable] = None
    ) -> Dict[str, Any]:
        """
        Main scraping method
        
        Args:
            max_posts: Maximum number of posts to scrape
            sort_type: Sort type (hot/new/top/best/rising)
            keywords: Optional keywords for search strategy
            progress_callback: Progress callback function
            
        Returns:
            Dict with scraping results
        """
        print(f"ğŸš€ å¼€å§‹ Reddit çˆ¬å–")
        print(f"ğŸ“Š ç›®æ ‡: {max_posts} ä¸ªå¸–å­")
        print("=" * 60)
        
        start_time = time.time()
        
        # Step 1: Fetch post URLs
        if self.is_user_mode:
            post_urls = self.user_scraper.fetch_user_posts(
                username=self.target_name,
                max_posts=max_posts,
                progress_callback=progress_callback
            )
        else:
            post_urls = self.subreddit_scraper.fetch_subreddit_posts(
                subreddit=self.target_name,
                max_posts=max_posts,
                sort_type=sort_type,
                progress_callback=progress_callback,
                keywords=keywords
            )
        
        if not post_urls:
            print("âŒ æ²¡æœ‰è·å–åˆ°å¸–å­URL")
            return {'status': 'error', 'message': 'No posts found'}
        
        print(f"\nğŸ“Š è·å–åˆ° {len(post_urls)} ä¸ªå€™é€‰å¸–å­URL")
        
        # Step 2: Filter already scraped posts
        print("ğŸ” è¿‡æ»¤å·²å­˜åœ¨çš„å¸–å­...")
        post_urls = self._filter_existing_posts(post_urls)
        print(f"âœ… å»é‡åå‰©ä½™ {len(post_urls)} ä¸ªæ–°å¸–å­")
        
        if len(post_urls) == 0:
            print("âš ï¸ æ‰€æœ‰å¸–å­éƒ½å·²å­˜åœ¨äºæ•°æ®åº“ä¸­")
            return {'status': 'success', 'scraped_count': 0, 'message': 'All posts already scraped'}
        
        # Step 3: Scrape post details
        print(f"\nğŸ¯ å¼€å§‹æŠ“å– {len(post_urls)} ä¸ªå¸–å­...")
        print("=" * 60)
        
        scraped_posts = []
        for i, (post_url, post_id) in enumerate(post_urls, 1):
            if self.scraped_count >= max_posts:
                print(f"\nğŸ‰ å·²è¾¾åˆ°ç›®æ ‡ï¼åœæ­¢å¤„ç†")
                break
            
            print(f"\n[{i}/{len(post_urls)}] å¤„ç†å¸–å­: {post_id}")
            
            result = self.post_scraper.scrape_post(post_url)
            
            if result and result.get('status') == 'success':
                # Extract post and comments from the result
                post_data = result['post']
                post_data['comments'] = result['comments']  # Add comments to post data
                
                scraped_posts.append(post_data)
                self.scraped_count += 1
                
                # Save to database
                if local_data_manager.save_post(post_data):
                    print(f"âœ… {post_data['title'][:50]}...")
                    print(f"    ğŸ‘¤ ä½œè€…: {post_data['author']} | ğŸ“ˆ åˆ†æ•°: {post_data['score']} | ğŸ’¬ è¯„è®º: {len(post_data['comments'])}")
                else:
                    print(f"âŒ ä¿å­˜å¤±è´¥")
                
                if progress_callback:
                    try:
                        progress_callback(self.scraped_count, max_posts, f"Scraped: {post_data['title'][:30]}...")
                    except:
                        pass
            
            # Rate limiting
            if i < len(post_urls):
                delay = self.rate_controller.get_delay()
                time.sleep(delay)
        
        # Print final stats
        elapsed_time = time.time() - start_time
        print("\n" + "=" * 60)
        print("ğŸ‰ çˆ¬å–å®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   âœ… æˆåŠŸæŠ“å–: {self.scraped_count} ä¸ªå¸–å­")
        print(f"   â±ï¸  æ€»è€—æ—¶: {elapsed_time/60:.1f} åˆ†é’Ÿ")
        print(f"   âš¡ å¹³å‡é€Ÿåº¦: {elapsed_time/max(1, self.scraped_count):.1f} ç§’/å¸–")
        print(f"   ğŸ—„ï¸  æ•°æ®åº“æ€»å¸–å­æ•°: {local_data_manager.get_posts_count()}")
        print("=" * 60)
        
        # Auto-export to Markdown with user comment filter
        print("\nğŸ“ è‡ªåŠ¨å¯¼å‡ºMarkdown...")
        try:
            from export_to_markdown import export_to_markdown
            json_dir = local_data_manager.json_dir
            export_file = local_data_manager.current_session_dir + "/posts_export_filtered.md"
            
            # Extract username from target_name
            username = self.target_name
            
            export_to_markdown(json_dir, export_file, filter_username=username)
            print(f"âœ… å·²å¯¼å‡ºè¿‡æ»¤ç‰ˆæœ¬ï¼ˆåªåŒ…å« u/{username} çš„è¯„è®ºï¼‰")
            print(f"ğŸ“„ æ–‡ä»¶ä½ç½®: {export_file}")
        except Exception as e:
            print(f"âš ï¸ å¯¼å‡ºå¤±è´¥: {e}")
        
        return {
            'status': 'success',
            'scraped_count': self.scraped_count,
            'total_posts_in_db': local_data_manager.get_posts_count(),
            'elapsed_time': elapsed_time
        }
    
    def _filter_existing_posts(self, post_urls: List[Tuple[str, str]], batch_size: int = 100) -> List[Tuple[str, str]]:
        """Filter out posts that already exist in database"""
        new_posts = []
        
        for i in range(0, len(post_urls), batch_size):
            batch = post_urls[i:i + batch_size]
            post_ids = [post_id for _, post_id in batch]
            
            existing_ids = set()
            for post_id in post_ids:
                if local_data_manager.check_post_exists(post_id):
                    existing_ids.add(post_id)
            
            for post_url, post_id in batch:
                if post_id not in existing_ids:
                    new_posts.append((post_url, post_id))
        
        return new_posts


# Module-level convenience function for easy integration
def scrape_reddit(
    target: str,
    max_posts: int = 100,
    sort_type: str = 'hot',
    keywords: Optional[List[str]] = None,
    progress_callback: Optional[Callable] = None
) -> Dict[str, Any]:
    """
    Convenience function to scrape Reddit
    
    Args:
        target: Subreddit or user to scrape (r/python or u/spez)
        max_posts: Maximum posts to scrape
        sort_type: Sort type
        keywords: Optional search keywords
        progress_callback: Progress callback
        
    Returns:
        Dict with scraping results
    """
    scraper = RedditScraper(target)
    return scraper.scrape(max_posts, sort_type, keywords, progress_callback)


if __name__ == "__main__":
    print("ğŸ¤– Reddit é€šç”¨çˆ¬è™«")
    print("=" * 60)
    
    target = input("è¯·è¾“å…¥ç›®æ ‡ (r/subreddit æˆ– u/username): ").strip()
    max_posts = int(input("è¯·è¾“å…¥è¦çˆ¬å–çš„å¸–å­æ•° (é»˜è®¤100): ") or "100")
    
    scraper = RedditScraper(target)
    result = scraper.scrape(max_posts=max_posts)
    
    print(f"\næœ€ç»ˆç»“æœ: {result}")
